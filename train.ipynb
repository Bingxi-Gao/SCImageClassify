{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import model\n",
    "from utils import *\n",
    "from cnn_model import CNNModel\n",
    "\n",
    "## 定义交叉验证折数\n",
    "num_folds = 10\n",
    "\n",
    "# 初始化列表以存储结果\n",
    "all_train_acc = []\n",
    "all_train_loss = []\n",
    "all_lr = []\n",
    "all_test_acc = []\n",
    "all_test_loss = []\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "all_f1 = []\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "\n",
    "# 执行 k-fold 交叉验证\n",
    "dataset_splits = kfold_split_data(root=\"/home/WuHX/gbx/SMA/DMVCimage\", k=num_folds)\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    print(f\"Running fold {fold + 1}...\")\n",
    "\n",
    "    # 获取当前折的训练集和验证集数据拆分\n",
    "    train_images_path, train_images_label, val_images_path, val_images_label = dataset_splits[fold]\n",
    "\n",
    "    # 定义数据转换\n",
    "    img_size = 224\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "        \"val\": transforms.Compose([\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # 创建训练集和验证集\n",
    "    train_dataset = MyDataSet(images_path=train_images_path,\n",
    "                              images_class=train_images_label,\n",
    "                              transform=data_transform[\"train\"])\n",
    "\n",
    "    val_dataset = MyDataSet(images_path=val_images_path,\n",
    "                            images_class=val_images_label,\n",
    "                            transform=data_transform[\"val\"])\n",
    "\n",
    "    val_num = len(val_dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True,\n",
    "                                               num_workers=num_workers,\n",
    "                                               collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True,\n",
    "                                             num_workers=num_workers,\n",
    "                                             collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "    # 定义模型EARFGGRFDRHTG\n",
    "    net = model.resnet50(num_classes=8)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "    lr_scheduler = create_lr_scheduler(optimizer, len(train_loader), 50,\n",
    "                                       warmup=True, warmup_epochs=10)\n",
    "\n",
    "    # 训练循环\n",
    "    epochs = 500\n",
    "    fold_train_acc = []\n",
    "    fold_train_loss = []\n",
    "    fold_lr = []\n",
    "    fold_test_acc = []\n",
    "    fold_test_loss = []\n",
    "    fold_precision = []\n",
    "    fold_recall = []\n",
    "    fold_f1 = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        weight_decay = 0.0001\n",
    "        train_loss, train_acc, train_lr = train_one_epoch_classify(net, optimizer, train_loader, device, epoch,\n",
    "                                                                   lr_scheduler, weight_decay)\n",
    "\n",
    "        fold_train_acc.append(train_acc)\n",
    "        fold_train_loss.append(train_loss)\n",
    "        fold_lr.append(train_lr)\n",
    "\n",
    "        accu_loss, accu_num, all_pred, all_prob = evaluate_classify(net,\n",
    "                                                                    val_loader,\n",
    "                                                                    device,\n",
    "                                                                    epoch)\n",
    "        fold_test_acc.append(accu_num)\n",
    "        fold_test_loss.append(accu_loss)\n",
    "\n",
    "        # 计算 Precision、Recall 和 F1 得分\n",
    "        precision = precision_score(val_images_label, all_pred, average='macro')\n",
    "        recall = recall_score(val_images_label, all_pred, average='macro')\n",
    "        f1 = f1_score(val_images_label, all_pred, average='macro')\n",
    "        fold_precision.append(precision)\n",
    "        fold_recall.append(recall)\n",
    "        fold_f1.append(f1)\n",
    "\n",
    "    all_train_acc.append(fold_train_acc)\n",
    "    all_train_loss.append(fold_train_loss)\n",
    "    all_lr.append(fold_lr)\n",
    "    all_test_acc.append(fold_test_acc)\n",
    "    all_test_loss.append(fold_test_loss)\n",
    "    all_precision.append(fold_precision)\n",
    "    all_recall.append(fold_recall)\n",
    "    all_f1.append(fold_f1)\n",
    "\n",
    "# 计算所有折的平均结果\n",
    "avg_train_acc = np.mean(all_train_acc, axis=0)\n",
    "avg_train_loss = np.mean(all_train_loss, axis=0)\n",
    "avg_lr = np.mean(all_lr, axis=0)\n",
    "avg_test_acc = np.mean(all_test_acc, axis=0)\n",
    "avg_test_loss = np.mean(all_test_loss, axis=0)\n",
    "avg_precision = np.mean(all_precision, axis=0)\n",
    "avg_recall = np.mean(all_recall, axis=0)\n",
    "avg_f1 = np.mean(all_f1, axis=0)\n",
    "# Take the maximum values for each fold\n",
    "\n",
    "\n",
    "avg_train_acc_str = ', '.join(str(x) for x in avg_train_acc)\n",
    "avg_train_loss_str = ', '.join(str(x) for x in avg_train_loss)\n",
    "avg_lr_str = ', '.join(str(x) for x in avg_lr)\n",
    "avg_test_acc_str = ', '.join(str(x) for x in avg_test_acc)\n",
    "avg_test_loss_str = ', '.join(str(x) for x in avg_test_loss)\n",
    "avg_precision_str = ', '.join(str(x) for x in avg_precision)\n",
    "avg_recall_str = ', '.join(str(x) for x in avg_recall)\n",
    "avg_f1_str = ', '.join(str(x) for x in avg_f1)\n",
    "\n",
    "\n",
    "max_test_acc = np.max(all_test_acc, axis=1)\n",
    "avg_max_test_acc = np.mean(max_test_acc)\n",
    "\n",
    "###先取出10折对应的10个列表，然后对10个列表中的对应位置的值取平均值，输出一个（50*1）的列表，可以看出模型效果的平均变化趋势\n",
    "\n",
    "print(\"all_test_acc =[\", avg_test_acc_str,\"]\")\n",
    "print(\"all_test_loss =[\", avg_test_loss_str,\"]\")\n",
    "print(\"all_precision =[\", avg_precision_str,\"]\")\n",
    "print(\"all_recall = [\", avg_recall_str,\"]\")\n",
    "print(\"all_f1 =[\", avg_f1_str,\"]\")\n",
    "\n",
    "###先取出10个列表中的最大值，然后对取出的10个最大值取平均值，可以看出模型的最优结果\n",
    "print(\"Average Maximum Test Accuracy:\", avg_max_test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
